# CodingLLM
In this project, we fine-tuned a pre-trained model using coding Q&A datasets for the code generation task. Specifically, we employed CodeT5 model, which was introduced in the paper [CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation by Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi](https://arxiv.org/abs/2109.00859). 

For training, we utilized two datasets:[Stack Overflow Dataset](https://archive.org/details/stackexchange) and [Iamtarun Python Code Instrutions 18k Alpaca](https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca). These datasets provide a rich source of coding questions and answers, serving as the foundation for fine-tuning the model's performance on real-world code generation tasks.
